{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Análise Sentimento Tweets LSTM Binary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonatapaulino/Recurrent-Neural-Network-LSTM/blob/master/Ana%CC%81lise_Sentimento_Tweets_LSTM_Binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqE57TLE9XPq"
      },
      "source": [
        "## Montando o ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkdS-FAT6r5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dada45b-8397-4140-a422-00261e2c6851"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZa841a6jge"
      },
      "source": [
        "## Importações dos pacotes necessários"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxixvvo46jgh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "48a25f15-ac28-4e67-a2c5-4aed16d59d37"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.models import Model\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "#from keras import backend\n",
        "\n",
        "#import re, os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "#import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNl2ptRs6jgo"
      },
      "source": [
        "## Configurações Iniciais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLtszJBc6jgp"
      },
      "source": [
        "seed = 7\n",
        "seed = np.random.seed(seed)\n",
        "\n",
        "# O model será exportado para este arquivo\n",
        "#filename='C:/analisesentimentoLSTM/model/model_saved.h5'\n",
        "# filename='/content/drive/My Drive/Redes LSTM FINAL/model/model_saved1.h5'\n",
        "filename= '/content/drive/My Drive/Redes LSTM FINAL/model/model_saved.h5'\n",
        "\n",
        "epochs = 60\n",
        "\n",
        "# dimensionalidade do word embedding pré-treinado\n",
        "word_embedding_dim = 50\n",
        "\n",
        "# número de amostras a serem utilizadas em cada atualização do gradiente\n",
        "batch_size = 40\n",
        "\n",
        "# Reflete a quantidade máxima de palavras que iremos manter no vocabulário\n",
        "max_fatures = 6000\n",
        "\n",
        "# dimensão de saída da camada Embedding\n",
        "embed_dim = 128\n",
        "\n",
        "# limitamos o tamanho máximo de todas as sentenças\n",
        "max_sequence_length = 300\n",
        "\n",
        "pre_trained_wv = True\n",
        "\n",
        "bilstm = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kWc0u96jgt"
      },
      "source": [
        "## Funções"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7AXRRaQ6jgu"
      },
      "source": [
        "# Função para limpar os dados\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    string = re.sub(r'\\d+', '', string)\n",
        "    string = re.sub(cleanr, '', string)\n",
        "    string = re.sub(\"'\", '', string)\n",
        "    string = re.sub(r'\\W+', ' ', string)\n",
        "    string = string.replace('_', '')\n",
        "\n",
        "    return string.strip().lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkLYI4XF6jgy"
      },
      "source": [
        "# Função de preparação dos dados\n",
        "def prepare_data(data):\n",
        "    \n",
        "    data = data[['texto', 'sentimento']]\n",
        "\n",
        "    data['texto'] = data['texto'].apply(lambda x: x.lower())\n",
        "    data['texto'] = data['texto'].apply(lambda x: clean_str(x))\n",
        "    data['texto'] = data['texto'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "    \n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    text = []\n",
        "    for row in data[\"texto\"].values:\n",
        "        word_list = text_to_word_sequence(row)\n",
        "        no_stop_words = [w for w in word_list if not w in stop_words]\n",
        "        no_stop_words = \" \".join(no_stop_words)\n",
        "        text.append(no_stop_words)\n",
        "\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    X = tokenizer.texts_to_sequences(text)  \n",
        "    \n",
        "    X = pad_sequences(X, maxlen=max_sequence_length)\n",
        "    #X = pad_sequences(X)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    Y = pd.get_dummies(data['sentimento']).values\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test, word_index, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGmI4wrb6jg2"
      },
      "source": [
        "def load_pre_trained_wv(word_index, num_words, word_embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join('/content/drive/My Drive/Redes LSTM FINAL/word_embedding/glove.6B.50d.txt'.format(word_embedding_dim)), encoding='utf-8')\n",
        "    for line in tqdm(f):\n",
        "        values = line.rstrip().rsplit(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    print('%s word vectors.' % len(embeddings_index))\n",
        "\n",
        "    embedding_matrix = np.zeros((num_words, word_embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_fatures:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz7r7t2q6jg7"
      },
      "source": [
        "### Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhzywSRa6jg8"
      },
      "source": [
        "def build_model():\n",
        "    if pre_trained_wv is True:\n",
        "        print(\"USE PRE TRAINED\")\n",
        "        num_words = min(max_fatures, len(word_index) + 1)\n",
        "        weights_embedding_matrix = load_pre_trained_wv(word_index, num_words, word_embedding_dim)\n",
        "        input_shape = (max_sequence_length,)\n",
        "        model_input = Input(shape=input_shape, name=\"input\", dtype='int32')    \n",
        "        embedding = Embedding(\n",
        "            num_words, \n",
        "            word_embedding_dim,\n",
        "            input_length=max_sequence_length, \n",
        "            name=\"embedding\", \n",
        "            weights=[weights_embedding_matrix], \n",
        "            trainable=False)(model_input)\n",
        "        if bilstm is True:\n",
        "            lstm = Bidirectional(LSTM(word_embedding_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\"))(embedding)\n",
        "        else:\n",
        "            lstm = LSTM(word_embedding_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\")(embedding)\n",
        "\n",
        "    else:\n",
        "        input_shape = (max_sequence_length,)\n",
        "        model_input = Input(shape=input_shape, name=\"input\", dtype='int32')    \n",
        "\n",
        "        embedding = Embedding(max_fatures, embed_dim, input_length=max_sequence_length, name=\"embedding\")(model_input)\n",
        "        \n",
        "        if bilstm is True:\n",
        "            lstm = Bidirectional(LSTM(embed_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\"))(embedding)\n",
        "        else:\n",
        "            lstm = LSTM(embed_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\")(embedding)\n",
        "    \n",
        "    model_output = Dense(2, activation='softmax', name=\"softmax\")(lstm)\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b_mY1v6jhA"
      },
      "source": [
        "### Funções da F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bAJrSwr6jhB"
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNPhuJtP6jhF"
      },
      "source": [
        "## Execução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Atws6v6jhG"
      },
      "source": [
        "### Leitura de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DwguIQ26jhH"
      },
      "source": [
        "#data = pd.read_excel('./dataset/imdb.xlsx') # Lembre de instalar o pacote 'xlrd'\n",
        "#data = pd.read_excel('C:/analisesentimentoLSTM/basernrlstm2.xlsx')\n",
        "data = pd.read_excel('/content/drive/My Drive/Redes LSTM FINAL/basernrlstm2.xlsx')\n",
        "#data.drop_duplicates()\n",
        "\n",
        "X_train, X_test, Y_train, Y_test, word_index, tokenizer = prepare_data(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp3azeedicrB"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxDr1S5djBI6"
      },
      "source": [
        "#data.groupby(['sentimento']).sentimento.count().sort_values().plot(kind='bar', color=['c', 'b'])\n",
        "#plt.yticks([1, 1000, 2000, 3000])\n",
        "#plt.grid(True)\n",
        "sns.catplot(x=\"sentimento\", kind=\"count\", palette=\"Set2\", data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiOTsyr-6jhL"
      },
      "source": [
        "### Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkItfXwT6jhM"
      },
      "source": [
        "Criação ou leitura do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btbsMqxC6jhM"
      },
      "source": [
        "# Se existir um modelo salvo, o programa irá ler ele\n",
        "if not os.path.exists('./{}'.format(filename) ):\n",
        "    # Criação do modelo\n",
        "    modelo = build_model()\n",
        "    modelo.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "                   metrics=['accuracy', f1_m])\n",
        "    #model.save_weights(filename)\n",
        "else:\n",
        "    model.load_weights('./{}'.format(filename))\n",
        "\n",
        "# metrics=['accuracy', f1_m])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_Q_oU9W6jhR"
      },
      "source": [
        "\n",
        "Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9xshdn76jhT"
      },
      "source": [
        "hist = modelo.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "                  epochs=epochs, batch_size=batch_size, shuffle=True,\n",
        "                  verbose=1)\n",
        "\n",
        "scores = modelo.evaluate(X_test, Y_test, verbose=0,\n",
        "                         batch_size=batch_size)\n",
        "\n",
        "print(\"Acurácia: %.2f%%\" % (scores[1]*100))\n",
        "# print(\"f1_score: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aK5ezg_6jhd"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufIkKN_26jhe"
      },
      "source": [
        "### Cross-Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmiFviUz6jhf"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
        "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
        "plt.title('Criminals classifier')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross-Entropy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Cross-Entropy')\n",
        "figure = plt.gcf()\n",
        "plt.show()\n",
        "figure.savefig('/content/drive/My Drive/Redes LSTM FINAL/imagens gráficos/perda_cross.png', format='png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Tnrtip6jhk"
      },
      "source": [
        "### F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1asVT3S6jhl"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(hist.history['f1_m'], lw=2.0, color='b', label='train')\n",
        "plt.plot(hist.history['val_accuracy'], lw=2.0, color='r', label='val')\n",
        "plt.title('Criminals classifier')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('f1_score')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('F1 Score')\n",
        "figure = plt.gcf()\n",
        "plt.show()\n",
        "figure.savefig('/content/drive/My Drive/Redes LSTM FINAL/imagens gráficos/f1_cross.png', format='png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N46BEDJ6jhr"
      },
      "source": [
        "### Curva ROC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jSHBea86jhs"
      },
      "source": [
        "Y_pred = modelo.predict(X_test)\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(2):\n",
        "    fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], Y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr[1], tpr[1], color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "figure = plt.gcf()\n",
        "plt.show()\n",
        "figure.savefig('/content/drive/My Drive/Redes LSTM FINAL/imagens gráficos/roc_cross.png', format='png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAR6yBH6svpT"
      },
      "source": [
        "### Matriz de Confusão\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMscZQ2Ns--H"
      },
      "source": [
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "Y_pred = modelo.predict(X_test)\n",
        "Y_pred = np.round(Y_pred)\n",
        "Y_pred = Y_pred.astype(int)\n",
        "    \n",
        "Y_test_tmp = []\n",
        "for i in Y_test:\n",
        "    if i[0] == 0:\n",
        "        Y_test_tmp.append(0)\n",
        "    else:            \n",
        "        Y_test_tmp.append(1)\n",
        "\n",
        "Y_pred_tmp = []\n",
        "for i in Y_pred:\n",
        "    if i[0] == 0:\n",
        "        #Y_pred_tmp.append('crime')\n",
        "        Y_pred_tmp.append(0)\n",
        "    else:            \n",
        "        #Y_pred_tmp.append('naocrime')\n",
        "        Y_pred_tmp.append(1)\n",
        "        \n",
        "    #mc = confusion_matrix(Y_test_tmp, Y_pred_tmp)\n",
        "    #print (mc)\n",
        "    \n",
        "    #from sklearn.metrics import confusion_matrix\n",
        "\n",
        "labels = ['crime', 'naocrime']\n",
        "cm = confusion_matrix(Y_test_tmp, Y_pred_tmp)\n",
        "# 1-sns.heatmap(cm, annot=True)\n",
        "# 2- sns.heatmap(cm, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
        "# sns.heatmap(cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16})\n",
        "sns.heatmap(cm/np.sum(cm), annot=True, fmt=\".2%\", cmap=\"Blues\")\n",
        "#print(cm)\n",
        "#fig = plt.figure()\n",
        "#ax = fig.add_subplot(111)\n",
        "#cax = ax.matshow(cm)\n",
        "plt.title('Confusion matrix of the classifier')\n",
        "#fig.colorbar(cax)\n",
        "#ax.set_xticklabels([''] + labels)\n",
        "#ax.set_yticklabels([''] + labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "figure = plt.gcf()\n",
        "plt.show()\n",
        "figure.savefig('/content/drive/My Drive/Redes LSTM FINAL/imagens gráficos/matrix_cross.png', format='png')\n",
        "#============= Plot Matrix de confusão =============#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RHaDu_sC90w"
      },
      "source": [
        "### Arquitetura da Rede"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BeIavI1DkVw"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(modelo, \n",
        "           to_file='model_plot.png', \n",
        "           show_shapes=True, \n",
        "           show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvQWL8ZND8ZD"
      },
      "source": [
        "print(modelo.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgPgVL7SH5MP"
      },
      "source": [
        "# Nuvem de Palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5QZR_VI8qDB"
      },
      "source": [
        "!pip install wordcloud -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogvzbr-DIMWG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGRyb6J1IicR"
      },
      "source": [
        "df = pd.read_excel('/content/drive/My Drive/Redes LSTM FINAL/basernrlstm2_nuvem.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BkF4XckJKoa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "45852d0d-157b-492e-b32b-7d2fc7f59ac7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Homem mata companheira a facadas por não gosta...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Polícia Civil prende suspeito de matar mulher ...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Baterista é condenado por matar namorada com m...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alunos são atropelados durante protesto em São...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Polícia apura denúncia de incêndio criminoso e...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               texto sentimento\n",
              "0  Homem mata companheira a facadas por não gosta...      crime\n",
              "1  Polícia Civil prende suspeito de matar mulher ...      crime\n",
              "2  Baterista é condenado por matar namorada com m...      crime\n",
              "3  Alunos são atropelados durante protesto em São...      crime\n",
              "4  Polícia apura denúncia de incêndio criminoso e...      crime"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zUGU0u6JPtS"
      },
      "source": [
        "df.dropna(subset=['texto'], axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7huztxcIkTv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "60d27fb0-9be9-4218-f77d-f6f8e52efd38"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Homem mata companheira a facadas por não gosta...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Polícia Civil prende suspeito de matar mulher ...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Baterista é condenado por matar namorada com m...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alunos são atropelados durante protesto em São...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Polícia apura denúncia de incêndio criminoso e...</td>\n",
              "      <td>crime</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               texto sentimento\n",
              "0  Homem mata companheira a facadas por não gosta...      crime\n",
              "1  Polícia Civil prende suspeito de matar mulher ...      crime\n",
              "2  Baterista é condenado por matar namorada com m...      crime\n",
              "3  Alunos são atropelados durante protesto em São...      crime\n",
              "4  Polícia apura denúncia de incêndio criminoso e...      crime"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-S8IHS8JVoF"
      },
      "source": [
        "summary = df['texto']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWoN9aAaJVbg"
      },
      "source": [
        "all_summary = ''.join(s for s in summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKiJmGfBKGhB"
      },
      "source": [
        "stopwords = set(STOPWORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq8wyT9zKG6E"
      },
      "source": [
        "stopwords.update([\"da\", \"meu\", \"em\", \"você\", \"de\", \"ao\", \"os\", \"por que\", \"na\", \"por\", \"que\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hMo9eK4Kcul"
      },
      "source": [
        "wordcloud = WordCloud(stopwords=stopwords,\n",
        "                      background_color='black', width=1600,                            \n",
        "                      height=800).generate(all_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAxI_7PLKdUX"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(16,8))            \n",
        "ax.imshow(wordcloud, interpolation='bilinear')       \n",
        "ax.set_axis_off()\n",
        "plt.imshow(wordcloud)                 \n",
        "wordcloud.to_file('/content/drive/My Drive/Redes LSTM FINAL/imagens gráficos/lstm_nuvem.png');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQGznWDOFrtY"
      },
      "source": [
        "## Callbacks para Salvar Resultados Intermediários e Visualizar na Web com TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp5gHWRmFjqf"
      },
      "source": [
        "# Instale TensorBoard:\n",
        "#!pip install TensorBoard\n",
        "# Inicie o servidor de um terminal, indicando o diretório de leitura --logdir=~/logs/\n",
        "# Observe que nesta chamada estamos:\n",
        "# - redirecionando tanto stdio quanto stderr para ~/tensorboard.log,\n",
        "#   o tensorboard fica soltando um monte de mensagens, é interessante guardar.\n",
        "# - rodando como processo em background (não faz sentido ocupar um terminal)\n",
        "# tensorboard --logdir=~/logs/ >> ~/tensorboard.log 2>&1 &\n",
        "\n",
        "# Importe a infra de callbacks do Keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Se você deseja visualizar o progresso da sua rede no Browser,\n",
        "# importe também a infra de suporte ao TensorBoard da Google\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "# Crie um checkpoint na forma de um string parametrizável de nome de arquivo para uma rede.\n",
        "# Os parâmetros entre {}s serão substituídos por valores da rede no momento de salvamento:\n",
        "filepath = '~/minhaPastaDeRedes/nomeDaMinhaRede-model-improvement-{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        " \n",
        "# Crie um ponteiro para uma instância de uma função de callback com dados personalizados.\n",
        "# Neste caso: Uma instância de ModelCheckpoint para salvar a rede ou os seus pesos:\n",
        "# Faça salvar a rede inteira fazendo save_weights_only=False\n",
        "# Faça salvar apenas se houver um ganho absoluto global com save_best_only=True\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1,\n",
        "                             save_best_only=True, mode='max', save_weights_only=False)\n",
        " \n",
        "# Crie uma lista com os ponteiros para as funções de callback.\n",
        "# Neste caso será uma lsita unitária:\n",
        "callbacks_list = [checkpoint]\n",
        " \n",
        "# Inclua callbacks=callbacks_list, nos parâmetros de Model.fit() para que o callback funcione:\n",
        "history = modelo.fit_generator(\n",
        "   Y_train,\n",
        "   steps_per_epoch=Y_train/Y_train. ,\n",
        "   epochs=args.epochs,\n",
        "   validation_data=validation_generator,\n",
        "   validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "   callbacks=callbacks_list,\n",
        "   verbose=1)\n",
        "\n",
        "# Crie um ponteiro para uma instância de uma função de callback com dados personalizados.\n",
        "# Neste caso: Uma instância de TensorBoard para escrever o sattus do treinamento no formato do TensorBoard.\n",
        "# Indique o diretório de leitura e sintaxe do servidor de TensorBoard com log_dir=\"~/logs/{}\".format(time())\n",
        "tensorboard = TensorBoard(log_dir=\"~/logs/{}\".format(time()))\n",
        "\n",
        "# Se você estiver usando TensorBoard junto com o ModelCheckpoint do exemplo acima, a sua \n",
        "# lista de ponteiros para funções de callback terá dois elementos e será assim:\n",
        "callbacks_list = [checkpoint, tensorboard]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvu-9VlB9IHV"
      },
      "source": [
        "'''\n",
        "# Arquitetura da nossa rede\n",
        "from ann_visualizer.visualize import ann_viz;\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "model.load_weights(\"model.h5\")\n",
        "\n",
        "ann_viz(model, title=\"Artificial Neural network - Model Visualization\")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYq_p0-KPmqh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}